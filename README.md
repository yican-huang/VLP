# VLP
[toc]
# 多模态总结 1-12
## CLIP
**从自然语言监督中学习可迁移视觉模型**
《Learning Transferable Visual Models From Natural Language Supervision》
[![pFCqZ8O.png](https://s11.ax1x.com/2024/01/12/pFCqZ8O.png)](https://imgse.com/i/pFCqZ8O)
- 训练方式
模型的输入是若干个 图像-文本 对儿（如图最上面的数据中图像是一个小狗，文本是 ”Pepper the aussie pup”）
CLIP 的方法很简单，但效果却意外的好。CLIP 的迁移能力是非常强的，预训练好的模型能够在任意一个视觉分类的数据集上取得不错的效果，而且最重要的是它是 zero-shot 的，即完全没有在这些数据集上做训练就能得到这么高的性能。CLIP 打破了之前固定种类标签的方法彻底解除了视觉模型的固有训练过程，引发了一大批后续工作。作者做了大量实验，在许多数据集上 CLIP 的效果都很好，泛化能力也很强，甚至在一些领域比人类的 zero-shot 性能还好。CLIP 用一个模型就能解决大部分的分类任务，而且是 zero-shot 的方式，更何况只要利用好 CLIP 训练好的模型，再在其他领域里稍微适配一下，就能也很好的完成其他领域的任务。
- 对比学习
CLIP 就是在以上这些特征上去做对比学习，对比学习非常灵活，只需要正样本和负样本的定义，其它都是正常套路。这里配对的 图像-文本 对儿就是正样本（即图中对角线（蓝色）部分，配对的图像和文本所描述的是同一个东西，那么矩阵中剩下的所有不是对角线上的元素（图中白色部分）就是负样本了。有了正、负样本后，模型就可以通过对比学习的方式去训练，不需要任何手工的标注。对于这种无监督的预训练方式，如对比学习，是需要大量数据的，OpenAI专门去收集了这么一个数据集，其中有**4亿个 图像-文本对**，且数据清理的比较好，质量比较高，这也是CLIP如此强大的主要原因之一。
## VILT
**非常简洁的多模态方法ViLT**，可以理解为**BERT+ViT**。
[![pFCqczF.png](https://s11.ax1x.com/2024/01/12/pFCqczF.png)](https://imgse.com/i/pFCqczF)
文章给出的这个图，我觉得很有用，它将多模态（指的是图文两个模态）的网络大概分为以上的几种：分别将文本模态、图像模态的复杂度以及融合后是采用简单的点积还是一个可学习的模块。
现有的VLP模型的text embedding基本上都使用类似BERT结构，但是visual embedding存在着差异。在大多数情况下，visual embedding是现有VLP模型的瓶颈。visual embedding的方法总共有三大类，其中**region feature**方法通常采用Faster R-CNN二阶段检测器提取region的特征(如ViLBERT和UNITER)，**grid feature**方法直接使用CNN提取grid的特征(如Pixel-BERT)，**patch projection**方法将输入图片切片投影提取特征。ViLT是首个使用patch projection来做visual embedding的方法。
![](:/3a507ae005014fc292bd61d4a15e7479)
**目的**：提出一种简化的视觉和语言预训练（VLP）模型，称为视觉和语言变换器（ViLT），它不使用卷积神经网络或对象检测器来处理视觉输入，而是使用线性投影将图像块嵌入到变换器中。
**方法**：ViLT使用与文本相同的方式来嵌入和处理视觉输入，利用变换器的能力来提取和融合多模态特征。ViLT使用图像文本匹配和遮盖语言建模作为预训练目标，并使用整词遮盖和图像增强来提高泛化性能。
**结果**：ViLT在各种视觉和语言下游任务上达到了与其他VLP模型相当或更好的性能，同时具有显著的运行时和参数效率。ViLT是第一个在没有卷积或区域监督的情况下实现竞争性能的VLP模型。
## ALBEF
**基于动量蒸馏的视觉语言表示学习**
视觉语言预训练 (Vision-and-Language Pre-training,VLP) 的目标是从大规模**image-text对**中学习多模态表示，用于改善下游的视觉语言任务。许多现有的 VLP方法依赖于预训练的目标检测器来抽取基于图像特征的区域，并利用一个多模态编码去来将图像特征与单词特征进行融合。多模态编码器被训练来解决那些需要联合理解图像和文本的任务，例如：`masked language modeling`和`image-text matching`。虽然有效，但是这些 VLP \\text{VLP} VLP框架存在着几个关系的限制：(1) 图像特征和单词嵌入都处于自己的空间中，这使得多模态编码器学习建模他们的交互更具挑战性；(2) 目标检测器的标准和计算都很昂贵，因为其需要在预训练的时候人工标注`bounding box`，并且在推断时为高分辨率图像；(3) 广泛被使用的`image-text`数据集都是从网络上收集的并且存在大类噪音。
![](:/041c7533986043b8bedde0ec79e3f2d0)
ALBEF的网络架构如图所示，可以看出是比较简单的，可以这样理解：图像端就是一个12层的ViT-base模型，而文本端则是一个Bert模型，==值得注意的是，将Bert模型“砍”成两半，前半段是做文本的特征提取，后半段是做文本-图像的模态融合==。
它的两个贡献：1.align before fuse（利用了clip的对比loss）在融合图像和文本之前通过对比学习对它们进行对齐，从而提高了视觉语言表示的质量和效率。因为不再是使用的目标检测的预训练模型，而是端到端的可学习。2.数据清理，引入了动量蒸馏，一种自我训练方法，它利用动量模型生成**伪目标**作为额外的监督，以提高在噪声数据上的学习效果。

## VLMO
两个创新：1.模型上：mixture-of-modality-experts；2.训练上：分阶段训练
[![pFCLEes.png](https://s11.ax1x.com/2024/01/12/pFCLEes.png)](https://imgse.com/i/pFCLEes)
把dual-encoder（双塔结构）和fusion-encoder（单塔结构）结合
[![pFCLZoq.png](https://s11.ax1x.com/2024/01/12/pFCLZoq.png)](https://imgse.com/i/pFCLZoq)
### 启发总结
两个创新点都是有用的，且简单。只是改了ffn层为多个ffn选择，就能很好的提高模型的性能，他的效果是比ALBEF还要好的。且有意思的点是：在图像模态预训练的注意力权重在文本模态就可以表现得还不错。而反过来似乎并不好。
